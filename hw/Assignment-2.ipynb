{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83b8a794-093e-4925-9488-c54e8b5915a7",
   "metadata": {},
   "source": [
    "# CS345 Spring 2024 Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0058d12d-bfab-4c8c-a455-19e6392325e6",
   "metadata": {},
   "source": [
    "## Part 1 How well do Perceptrons classify\n",
    "\n",
    "Evaluate the perceptron algorithm (the implementation provided in the lecture notebook) on the Haberman's dataset provided in the last assignment and the Breast Cancer Wisconsin (diagnostic) dataset provided by Scikit. Compare its accuracy against the SVC implementation (use default values for C). Perform a tenfold cross-validation and report the average and standard deviation for each classifier on each dataset. Since you might not be able to get the perceptron code in the lecture notebook to play well with the Sklearn cross-validation code, you can implement the cross-validation code simply using nested for loops. Is there a classifier among the two that appears to perform better? Provide a discussion of the observations you see.\n",
    "\n",
    "Make sure to allow the perceptron algorithm to run for a sufficient number of epochs\n",
    "\n",
    "Reminder : [Notebook for cross validation](https://github.com/sarathsreedharan/CS345/blob/master/spring24/notebooks/module05_02_cross_validation.ipynb )\n",
    "\n",
    "Note: Please remember that the perceptron expects labels to be 1 and -1. If the dataset doesn't provide labels in that form, you must first convert it into that form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc3c2cf-0664-4dfa-bf00-47d0b7a23554",
   "metadata": {},
   "source": [
    "### Note on presenting the results\n",
    "You can consider using panda dataframes to present your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a219a6c-ffc4-4592-a7f4-d911d6f8d394",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'd:\\Program Files (x86)\\C++ Compiler\\ucrt64\\bin\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"d:/Program Files (x86)/C++ Compiler/ucrt64/bin/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "## Your code goes here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Haberman's dataset\n",
    "haberman_df = pd.read_csv('haberman.csv')\n",
    "X_haberman = haberman_df.drop('status', axis=1).values\n",
    "y_haberman = haberman_df['status'].values\n",
    "y_haberman = np.where(y_haberman == 2, -1, 1)  # Convert labels to -1 and 1\n",
    "\n",
    "# Load Breast Cancer Wisconsin dataset\n",
    "breast_cancer = load_breast_cancer()\n",
    "X_bc = breast_cancer.data\n",
    "y_bc = breast_cancer.target\n",
    "y_bc = np.where(y_bc == 0, -1, 1)  # Convert labels to -1 and 1\n",
    "\n",
    "# Preprocess data\n",
    "scaler = StandardScaler()\n",
    "X_bc = scaler.fit_transform(X_bc)\n",
    "\n",
    "class perceptron :\n",
    "    \"\"\"An implementation of the perceptron algorithm.\n",
    "    Note that this implementation does not include a bias term\"\"\"\n",
    " \n",
    "    def __init__(self, iterations=100, learning_rate=0.2, \n",
    "                 plot_data=False, random_w=False, seed=42) :\n",
    "        self.iterations = iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.plot_data = plot_data\n",
    "        self.random_w = random_w\n",
    "        self.seed = seed\n",
    "  \n",
    "    def fit(self, X, y) :\n",
    "        \"\"\"\n",
    "        Train a classifier using the perceptron training algorithm.\n",
    "        After training the attribute 'w' will contain the perceptron weight vector.\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    " \n",
    "        X : ndarray, shape (num_examples, n_features)\n",
    "        Training data.\n",
    " \n",
    "        y : ndarray, shape (n_examples,)\n",
    "        Array of labels.\n",
    " \n",
    "        \"\"\"\n",
    "        \n",
    "        if self.random_w :\n",
    "            rng = np.random.default_rng(self.seed)\n",
    "            self.w = rng.uniform(-1 , 1, len(X[0]))\n",
    "            print(\"initialized with random weight vector\")\n",
    "        else :\n",
    "            self.w = np.zeros(len(X[0]))\n",
    "            print(\"initialized with a zeros weight vector\")\n",
    "        self.wold = self.w\n",
    "        converged = False\n",
    "        iteration = 0\n",
    "        while (not converged and iteration <= self.iterations) :\n",
    "            converged = True\n",
    "            for i in range(len(X)) :\n",
    "                if y[i] * self.decision_function(X[i]) <= 0 :\n",
    "                    self.wold = self.w\n",
    "                    self.w = self.w + y[i] * self.learning_rate * X[i]\n",
    "                    converged = False\n",
    "                    if self.plot_data:\n",
    "                        self.plot_update(X, y, i)\n",
    "            iteration += 1\n",
    "        self.converged = converged\n",
    "        if converged :\n",
    "            print ('converged in %d iterations ' % iteration)\n",
    " \n",
    "    def decision_function(self, x) :\n",
    "        return np.dot(x, self.w)\n",
    " \n",
    "    def predict(self, X) :\n",
    "        \"\"\"\n",
    "        make predictions using a trained linear classifier\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    " \n",
    "        X : ndarray, shape (num_examples, n_features)\n",
    "        Training data.\n",
    "        \"\"\"\n",
    " \n",
    "        scores = np.dot(X, self.w)\n",
    "        return np.sign(scores)\n",
    "    \n",
    "    def plot_update(self, X, y, ipt) :\n",
    "        fig = plt.figure(figsize=(4,4))\n",
    "        plt.xlim(-1,1)\n",
    "        plt.ylim(-1,1)\n",
    "        plt.xlabel(\"Feature 1\")\n",
    "        plt.ylabel(\"Feature 2\")\n",
    "        plt.arrow(0,0,self.w[0],self.w[1], \n",
    "                  width=0.001,head_width=0.05, \n",
    "                  length_includes_head=True, alpha=1,\n",
    "                  linestyle='-',color='darkred')\n",
    "        plt.arrow(0,0,self.wold[0],self.wold[1], \n",
    "                  width=0.001,head_width=0.05, \n",
    "                  length_includes_head=True, alpha=1,\n",
    "                  linestyle='-',color='orange')\n",
    "        anew = -self.w[0]/self.w[1]\n",
    "        aold = -self.wold[0]/self.wold[1]\n",
    "        pts = np.linspace(-1,1)\n",
    "        plt.plot(pts, anew*pts, color='darkred')\n",
    "        plt.plot(pts, aold*pts, color='orange')\n",
    "        plt.title(\"in orange:  old w; in red:  new w\")\n",
    "        cols = {1: 'g', -1: 'b'}\n",
    "        for i in range(len(X)): \n",
    "            plt.plot(X[i][0], X[i][1], cols[y[i]]+'o', alpha=0.6,markersize=5) \n",
    "        plt.plot(X[ipt][0], X[ipt][1], 'ro', alpha=0.2,markersize=20)\n",
    "\n",
    "# Function to perform cross-validation\n",
    "def custom_cross_val_score(model, X, y, cv=10):\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=kf)\n",
    "    return scores.mean(), scores.std()\n",
    "\n",
    "# Instantiate classifiers\n",
    "perceptron = perceptron(iterations=100, learning_rate=0.01)\n",
    "svc = SVC()\n",
    "\n",
    "# Perform cross-validation\n",
    "haberman_perceptron_mean, haberman_perceptron_std = custom_cross_val_score(perceptron, X_haberman, y_haberman)\n",
    "haberman_svc_mean, haberman_svc_std = custom_cross_val_score(svc, X_haberman, y_haberman)\n",
    "\n",
    "bc_perceptron_mean, bc_perceptron_std = custom_cross_val_score(perceptron, X_bc, y_bc)\n",
    "bc_svc_mean, bc_svc_std = custom_cross_val_score(svc, X_bc, y_bc)\n",
    "\n",
    "# Print results\n",
    "print(\"Haberman's Dataset:\")\n",
    "print(\"Perceptron Accuracy: {:.2f} (+/- {:.2f})\".format(haberman_perceptron_mean, haberman_perceptron_std))\n",
    "print(\"SVC Accuracy: {:.2f} (+/- {:.2f})\".format(haberman_svc_mean, haberman_svc_std))\n",
    "\n",
    "print(\"\\nBreast Cancer Wisconsin Dataset:\")\n",
    "print(\"Perceptron Accuracy: {:.2f} (+/- {:.2f})\".format(bc_perceptron_mean, bc_perceptron_std))\n",
    "print(\"SVC Accuracy: {:.2f} (+/- {:.2f})\".format(bc_svc_mean, bc_svc_std))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a4dcb4-ce3d-49a2-ae16-916da669f39f",
   "metadata": {},
   "source": [
    "*Discussion of the results here*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3ce3b7-61bd-45f9-a775-e89fd8e69cd4",
   "metadata": {},
   "source": [
    "## Part 2 - Learning Curve\n",
    "\n",
    "We looked briefly at the idea of learning curves in the notebook for cross-validation. It looked at how accuracy changed with respect to the number of training examples. In this part of the assignment, plot a learning curve for the perceptron algorithm for an increasing number of training examples. To plot this, \n",
    "1. First, create a held-out validation set against which you will be comparing all your trained models\n",
    "2. Now, from the remaining data set, create train sets of increasing sizes. To create more compact plots you can use a logarithmic scale like 10, 20, 40, 80, 160, etc..\n",
    "\n",
    "For the plot X-axis should be the training data size considered and the Y-axis should be the accuracy of the model (obtained by training on the dataset of that size) as measured on the held-out validation set.  \n",
    "\n",
    "After receiving the plot, make sure to discuss your observations about the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c246835e-57dd-4cb3-ba38-5c2558ac6dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to generate the plot goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0ba3ee-ac92-45f5-92e0-8d149ccbbdce",
   "metadata": {},
   "source": [
    "*Discussion of the plot here*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b592783-24ac-4b98-a6fe-9273848f7a26",
   "metadata": {},
   "source": [
    "## Part 3 - Standardization\n",
    "\n",
    "Scaling your features is a core part of pre-processing data. One of the methods we have already seen in detail is that of [normalization](https://github.com/sarathsreedharan/CS345/blob/master/spring24/notebooks/module01_03_dot_products.ipynb). However, there are other methods. A popular one is called standardization. Under this method, you update each feature value such that it has zero mean and unit variation.\n",
    "\n",
    "You can find the details on how to do this at the following [wikipedia page](https://en.wikipedia.org/wiki/Feature_scaling#Standardization_(Z-score_Normalization)). Now, in this part, you have to implement the code to perform standardization (do not use the Sklearn function for it).\n",
    "\n",
    "1. Use that code on the Haberman's Dataset. Show how the mean of resultant feature values is zero and the standard deviation is one. Note that the method is applied across each individual feature. So, for a data matrix, the mean of the column is zero, and the standard deviation is one.\n",
    "2. Compare the accuracy obtained from Perceptron on Haberman's dataset, with and without standardization. As before, use tenfold cross-validation to compute accuracy and report the accuracy the same way you did for Part 1. Provide a small discussion on the results you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4ca7de-46a7-4250-89c4-a766b8ff6ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for standardization and it's application on the Haberman's dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d136f0c-ae96-4944-937d-71ae69fa8fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code to evaluate perceptron on a standardized and non-standardized dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11495b32-a0c0-42af-ba94-ca596286e62f",
   "metadata": {},
   "source": [
    "*Discussion of the results comparing perceptron on the two datset goes here*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3932759-db47-452a-af65-a28403e2eb12",
   "metadata": {},
   "source": [
    "### Grading \n",
    "\n",
    "Although we will not grade on a 100 pt scale, the following is a sample grading sheet that will give you a basic weightage of the different questions:  \n",
    "\n",
    "```\n",
    "Grading sheet for assignment 1\n",
    "\n",
    "Part 1:  40 points.\n",
    "  Fixing labels: 10\n",
    "  Cross-validation code: 20\n",
    "  Comparison, reporting, and discussion: 20\n",
    "Part 2:  20 points.\n",
    "  Creation of the learning curve: 10 points\n",
    "  Discussion of the plot: 10 points\n",
    "Part 3:  40 points\n",
    "  Standardization code and demonstrating it on the dataset: 20 points\n",
    "  Comparison, reporting, and discussion: 20 points\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
